---
title: "MD_HDT6"

---

```{r}
set.seed(1234)
datos <- read.csv("train.csv")
```

```{r echo=F, include=F, load_libraries}
library(dplyr)
library(cluster)
library(flexclust)
library(FeatureImpCluster)
library(tidyr)
library(stats)
library(ggrepel)
library(graphics)
library(NbClust)
library(factoextra)
library(profvis)
library(rpart)
library(mclust)
library(hopkins)
library(GGally)
library(corrplot)
library(caret)
library(ggplot2)
library(kableExtra)
library(dummy)
library(stringr)
library(e1071)
library(rpart.plot)
library(naivebayes)
library(randomForest)
library(mlr)
```

## División de variables numéricas y obtención de data de prueba y entrenamiento

#### División de variables
```{r}
numeric_variables <- c("SalePrice", "LotFrontage", "LotArea", "OverallQual", "OverallCond", "MasVnrArea", "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "X1stFlrSF", "X2ndFlrSF", "LowQualFinSF", "GrLivArea", "BsmtFullBath", "BsmtHalfBath", "FullBath", "HalfBath", "BedroomAbvGr", "KitchenAbvGr", "TotRmsAbvGrd", "Fireplaces", "GarageCars", "GarageArea", "WoodDeckSF", "OpenPorchSF", "EnclosedPorch", "X3SsnPorch", "ScreenPorch", "PoolArea", "MiscVal")
numericas <- datos[, numeric_variables]
cualitativas <- datos[, !(names(datos) %in% numeric_variables)]
cualitativas <- cualitativas[, !(names(cualitativas) %in% c("Id"))]
cualitativas <- cualitativas %>%
    mutate(MoSold = month.abb[MoSold])

datos <- datos %>% mutate_at(colnames(cualitativas), function(x) as.factor(x))
numericas <- datos[, numeric_variables]
datos <- datos[complete.cases(numericas), ]
numericas <- na.omit(numericas)
numericas_norm <- mutate_if(numericas, is.numeric, scale)
datos <- data.frame(numericas_norm, datos[, -match(numeric_variables, names(datos))])
```

#### Analisis de NA's

```{r}
faltantes_por_col <- colSums(is.na(datos))
faltantes_por_col
```
Como se puede observar en el conteo de datos faltantes, las columnas FireplaceQu, MiscFeature, Fence, Alley y PoolQC presentn varios datos faltantes. Es por esto que esas columnas no formaran parte de nuestro estudio

```{r}
datos <- select(datos, -Id, -PoolQC, -Fence, -Alley, -MiscFeature, -FireplaceQu) 
datos <- na.omit(datos)
```

#### Eliminación de variables no significativas

```{r}
datos <- select(datos, -GarageYrBlt, -SaleType, -SaleCondition, -MoSold, -YrSold, -YearBuilt, -Condition2, -MSSubClass, MSZoning, -LotShape)
```

En el dataset existian variables que solo agregaban ruido al estudio debido a la insignificancia de sus valores. Por lo tanto estas columnas fueron removidas.

### Creación de clasificación de la variable de precios

```{r}
p33 <- quantile(datos$SalePrice, 0.33)
p66 <- quantile(datos$SalePrice, 0.66)
datosT <- datos

datosT <- datosT %>%
    mutate(clasificacion = ifelse(datosT$SalePrice < p33, "Economicas",
        ifelse(datosT$SalePrice < p66, "Intermedias",
            "Caras"
        )
    ))
datosT$clasificacion <- as.factor(datosT$clasificacion)
```

### 1 Creación de variables dicotómicas

```{r}
library(fastDummies)
datos_con_dummy <- dummy_cols(datosT, select_columns = c("clasificacion"))
datos_con_dummy <- select(datos_con_dummy, -clasificacion, -clasificacion_Economicas, -clasificacion_Intermedias) 
datos_con_dummy$clasificacion_Caras <- datos_con_dummy$clasificacion_Caras
datos_con_dummy<-datos_con_dummy %>% mutate_at(c("clasificacion_Caras"),as.factor)
```

### 2 Use los mismos conjuntos de entrenamiento y prueba 

```{r warnings=FALSE}
porcentaje <- 0.7
set.seed(1234)

numeric_variables2 <- c("LotFrontage", "LotArea", "OverallQual", "OverallCond", "MasVnrArea", "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "X1stFlrSF", "X2ndFlrSF", "LowQualFinSF", "GrLivArea", "BsmtFullBath", "BsmtHalfBath", "FullBath", "HalfBath", "BedroomAbvGr", "KitchenAbvGr", "TotRmsAbvGrd", "Fireplaces", "GarageCars", "GarageArea", "WoodDeckSF", "OpenPorchSF", "EnclosedPorch", "X3SsnPorch", "ScreenPorch", "PoolArea", "MiscVal", "clasificacion_Caras")
datos_num <- select(datos_con_dummy, -SalePrice)
datos_num <- select(datos_con_dummy, -Utilities)
# datos_num <- datos_con_dummy[, numeric_variables2]

corte <- sample(nrow(datos_num), nrow(datos_num) * porcentaje)
train <- datos_num[corte, ]
test <- datos_num[-corte, ]
```

### 3 Elabore un modelo de regresion Logistica

```{r warning=FALSE, message=FALSE}

cv <- trainControl(method="cv", number=10)
modelo_todas_cv <- caret::train(clasificacion_Caras~., data=train,method="glm", family = binomial, trControl = cv)

```

```{r warnings=FALSE}
model_summary <- summary(modelo_todas_cv)
print(model_summary, signif.stars = TRUE, digits = 3)
```

Para representar la significancia el programa usa los *, de manera en que más * existan más significancia tiene. 

```{r warning=FALSE, message=FALSE}

variables_significativas <- model_summary$coefficients[model_summary$coefficients[, 4] < 0.01, , drop = FALSE]
columnas_significativas <- rownames(variables_significativas[order(abs(variables_significativas[, 1]), decreasing = TRUE), ])
columnas_significativas

```

Al usar todas las variables es posible notar que ninguna tiene una significancia, es por eso que se usaran unicamente las variables numericas. 

### 4.1 Análisis de correlación y ajuste de modelo

```{r warning=FALSE, message=FALSE}
datos_cor <- datos_con_dummy[, numeric_variables]
datos_cor <- select(datos_cor, -SalePrice)
correlacion <- cor(datos_cor)
corrplot(correlacion)
```

Existe correlación entre las siguientes variables:

- LotFrontage: LotArea, TotalBsmtSF, X1stFlrSF
- LotArea: LotFrontage, TotalBsmtSF, GrLivArea
- OverallQual: TotalBsmtSF, X1stFlrSF, GrLivArea, FullBath, GarageCars, GarageArea
- BsmtFinSF1: BsmtUnfSF, BsmtFullBath
- BsmtUnfSF: BsmtFinSF1
- TotalsmtSF: X1stFlrSF
- X1stFlrSF: TotalBsmtSF
- X2ndFlrSF: GrLivArea
- GrLivArea: X2ndFlrSF, TotRmsAbvGrd
- BsmtFullBath: BsmtFinSF1
- BedRoomAvGr: TotRmsAbvGr
- TotRmsAbvGrd: OverallQual, X2ndFlrSF, FullBath, BedroomAbvGr
- FirePlaces: OverallQual, X1stFlrSF, GrLivArea
- GarageCars: OverallQual, TotalBsmtSF, X1stFlrSF, GrLivArea, FullBath, TotRmsAbvGrd, GarageArea 
- GarageArea: OverallQual, TotalBsmtSF, X1stFlrSF, GrLivArea, FullBath, TotRmsAbvGrd, GarageCars


```{r prediccion_modelo_numericas, warning=FALSE, message=FALSE}
test_1 = select(test, -clasificacion_Caras)
pred <- predict(modelo_todas_cv,newdata = test_1)
```

```{r}
caret::confusionMatrix(as.factor(pred),as.factor(test$clasificacion_Caras))
```
El modelo  presento un accuracy de 0.86, una sensitividad de 0.86 y una especificidad de 0.85. Si vemos la matriz de confusión podemos ver que tiene problemas al clasificar los falsos negativos.

### 5 Utilice el modelo con el conjunto de prueba y determine la eficiencia del algoritmo para clasificar.


```{r curva_de_aprendizaje}
# train <- train[, numeric_variables2]
datos.task = makeClassifTask(data = train, target = "clasificacion_Caras")
rin2 = makeResampleDesc(method = "CV", iters = 10, predict = "both")
lrn = makeLearner("classif.multinom", predict.type = "prob", trace = FALSE)
lc2 = generateLearningCurveData(learners = lrn, task = datos.task,
                                percs = seq(0.1, 1, by = 0.1),
                                measures = list(ber, setAggregation(ber, train.mean)), resampling = rin2,
                                show.info = FALSE)
plotLearningCurve(lc2, facet = "learner")
```

Al obserevar la grafica se puede observar que la curva de error balanceada de training siempre está en 0 y nunca converge con la curva de error balanceada de test. A partir de esto se puede concluir que el modelo si tiene sobreajuste. Esto también se puede observar en el accuracy, ya que es muy alto y se estan manejando pocas variables significativas.


### 6 Explique si hay sobreajuste o no

```{r warning=FALSE, message=FALSE}
porcentaje <- 0.7
set.seed(1234)

numeric_variables2 <- c("LotFrontage", "LotArea", "OverallQual", "OverallCond", "MasVnrArea", "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "X1stFlrSF", "X2ndFlrSF", "LowQualFinSF", "GrLivArea", "BsmtFullBath", "BsmtHalfBath", "FullBath", "HalfBath", "BedroomAbvGr", "KitchenAbvGr", "TotRmsAbvGrd", "Fireplaces", "GarageCars", "GarageArea", "WoodDeckSF", "OpenPorchSF", "EnclosedPorch", "X3SsnPorch", "ScreenPorch", "PoolArea", "MiscVal", "clasificacion_Caras")
# datos_num <- select(datos_con_dummy, -SalePrice)
# datos_num <- select(datos_con_dummy, -Utilities)
datos_num <- datos_con_dummy[, numeric_variables2]

corte <- sample(nrow(datos_num), nrow(datos_num) * porcentaje)
train <- datos_num[corte, ]
test <- datos_num[-corte, ]
```

```{r}
Rprof(memory.profiling = TRUE)
cv <- trainControl(method="cv", number=10)
modelo_todas_cv <- caret::train(clasificacion_Caras~., data=train,method="glm", family = binomial, trControl = cv)
Rprof(NULL)
pm1 <- summaryRprof(memory = "both")

```

```{r}
model_summary <- summary(modelo_todas_cv)
print(model_summary, signif.stars = TRUE, digits = 3)
```

Para obtener las variables con mayor significancia se tomo en cuenta todos las variables que tuvieran un valor menor a 0.01. Proporcionando las siguientes variables

```{r}

variables_significativas <- model_summary$coefficients[model_summary$coefficients[, 4] < 0.01, , drop = FALSE]
columnas_significativas <- rownames(variables_significativas[order(abs(variables_significativas[, 1]), decreasing = TRUE), ])
columnas_significativas

```

Se puede observar que hay cuatro variables significativas en el modelo. A pesar de haber eliminado columnas con poca significancia en el modelo, existen variables que siguen sin mostrar mayor significancia en el modelo.

####  6.1 Análisis de ajuste de modelo

```{r}
test_1 = select(test, -clasificacion_Caras)
pred <- predict(modelo_todas_cv,newdata = test_1)
```

```{r}
caret::confusionMatrix(as.factor(pred),as.factor(test$clasificacion_Caras))
```

Se obtuvo un accuracy de 0.887 y una sensibilidad de 0.9 y una especificidad de 0.82. Esto quiere decir que el modelo tiene un buen ajuste.

```{r}
datos.task = makeClassifTask(data = train, target = "clasificacion_Caras")
rin2 = makeResampleDesc(method = "CV", iters = 10, predict = "both")
lrn = makeLearner("classif.multinom", predict.type = "prob", trace = FALSE)
lc2 = generateLearningCurveData(learners = lrn, task = datos.task,
                                percs = seq(0.1, 1, by = 0.1),
                                measures = list(ber, setAggregation(ber, train.mean)), resampling = rin2,
                                show.info = FALSE)
plotLearningCurve(lc2, facet = "learner")
```

Se puede observar que el modelo no tiene sobreajuste tanto por la gráfica de curva de aprendizaje como por el accuracy de 0.88. Se puede obserevar que ambas curvas de balance de error convergen en cierto punto y se quedan relativamente constantes. 

### 7 Elaboración de modelos distintos

#### 7.1 Primer Modelo cambiando las varaibles predictorias 

Para este primer modelo se tomo en cuenta todas las variables que demostraban una significancia menores a 0.01. El valor del accuracy obtenido fue de 0.89, exactamente igual al mismo valor del modeloa anterior. 
```{r}
    porcentaje <- 0.7
    set.seed(1234)

    numeric_variables3 <- c("OverallQual","BsmtFinSF1","BsmtUnfSF", "X2ndFlrSF","BsmtFullBath", "FullBath", "PoolArea", "MiscVal", "clasificacion_Caras")
    datos_num <- datos_con_dummy[, numeric_variables3]

    corte <- sample(nrow(datos_num), nrow(datos_num) * porcentaje)
    train <- datos_num[corte, ]
    test <- datos_num[-corte, ]
    test_3 = select(test, -clasificacion_Caras)

    Rprof(memory.profiling = TRUE)
    cv <- trainControl(method="cv", number=10)
    modelo_cammbio1 <- caret::train(clasificacion_Caras~., data=train,method="glm", family = binomial, trControl = cv)
    Rprof(NULL)
    pm2 <- summaryRprof(memory = "both")

    model_summary <- summary(modelo_cammbio1)
    print(model_summary, signif.stars = TRUE, digits = 3)
```

```{r}
datos.task = makeClassifTask(data = train, target = "clasificacion_Caras")
rin2 = makeResampleDesc(method = "CV", iters = 10, predict = "both")
lrn = makeLearner("classif.multinom", predict.type = "prob", trace = FALSE)
lc2 = generateLearningCurveData(learners = lrn, task = datos.task,
                                percs = seq(0.1, 1, by = 0.1),
                                measures = list(ber, setAggregation(ber, train.mean)), resampling = rin2,
                                show.info = FALSE)
plotLearningCurve(lc2, facet = "learner")
```




#### 7.2 Segundo Modelo cambiando las varaibles predictorias 

```{r}
    porcentaje <- 0.7
    set.seed(1234)

    numeric_variables4 <- c("OverallQual","BsmtUnfSF", "FullBath", "PoolArea", "clasificacion_Caras")
    datos_num <- datos_con_dummy[, numeric_variables4]

    corte <- sample(nrow(datos_num), nrow(datos_num) * porcentaje)
    train <- datos_num[corte, ]
    test <- datos_num[-corte, ]
    test_4 = select(test, -clasificacion_Caras)

    Rprof(memory.profiling = TRUE)
    cv <- trainControl(method="cv", number=10)
    modelo_cammbio2 <- caret::train(clasificacion_Caras~., data=train,method="glm", family = binomial, trControl = cv)
    Rprof(NULL)
    pm3 <- summaryRprof(memory = "both")
    model_summary <- summary(modelo_cammbio2)
    print(model_summary, signif.stars = TRUE, digits = 3)
```

```{r}
datos.task = makeClassifTask(data = train, target = "clasificacion_Caras")
rin2 = makeResampleDesc(method = "CV", iters = 10, predict = "both")
lrn = makeLearner("classif.multinom", predict.type = "prob", trace = FALSE)
lc2 = generateLearningCurveData(learners = lrn, task = datos.task,
                                percs = seq(0.1, 1, by = 0.1),
                                measures = list(ber, setAggregation(ber, train.mean)), resampling = rin2,
                                show.info = FALSE)
plotLearningCurve(lc2, facet = "learner")
```

Podemos obsservar que las curvas de aprendizaje de test tiene un caida dramatica tratando de acercarse a la curva de train. Luego las  curvas se intersectan en 74% y 92% de los datos. Esto indica que el modelo tiene overfitting. Estos nos indica que utilizar solo las varaibles con significancia menor a 0.01 no es una buena opción para este modelo.

### 8 Análisis de la eficiencia de los modelos

#### 8.1 Modelo 1

```{r}
    caret::confusionMatrix(as.factor(pred),as.factor(test$clasificacion_Caras))
```


En este primer modelo tuvo un accuracy de 0.89. Este modelo se confundio mas en los falsos positivos y se confundio menos en los falsos negativos. Es decir que este modelo tiene problemas clasificando casas que son caras y marcandolas como si no fueran caras.
El tiempo total de este modelo fue de `r pm1$sampling.time` ms y su consumo total de memoria fue `r sum(pm1$by.total$mem.total)` MB

#### 8.2 Modelo 2
```{r}

pred1 <- predict(modelo_cammbio1,newdata = test_3)
caret::confusionMatrix(as.factor(pred1),as.factor(test$clasificacion_Caras))
```


En este segundo modelo tuvo un accuracy de 0.89 tambien. De igual manera que el modelo anterior, este modelo se confundio mas en los falsos negativos y se confundio menos en los falsos negativos. Es decir que este modelo tiene problemas clasificando casas que son caras y marcandolas como si no fueran caras.
El tiempo total de este modelo fue de `r pm2$sampling.time` ms y su consumo total de memoria fue `r sum(pm2$by.total$mem.total)` MB

#### 8.3 Modelo 3
```{r}

pred2 <- predict(modelo_cammbio2,newdata = test_4)
caret::confusionMatrix(as.factor(pred2),as.factor(test$clasificacion_Caras))

```
En este primer modelo tuvo un accuracy de 0.84. Este modelo se confundio mas en los falsos negativos y se confundio menos en los falsos negativos. Es decir que este modelo tiene problemas clasificando casas que son caras y marcandolas como si no fueran caras.
El tiempo total de este modelo fue de `r pm3$sampling.time` ms y su consumo total de memoria fue `r sum(pm3$by.total$mem.total)` MB


### 9 Determine cual de los 3 modelos fue mejor
Podemos ver que los dos nuevos modelos en los cuales se utilizaron las variables con significancia, el AIC es muy bajo en comparacion al primer modelo. Pero estos modelos muestran indicioso de overfitting en las graficas de curvas de aprendizaje, esto nos lleva a que no vale la pena evaluar el desempeño del profiler pues el overffiting opaca las otras caracteristicas . Por lo que consideramos que primer modelo es el mejor modelo ya que tiene un Accuracy de 0.89 y no muestra indicios de overfitting.

### 10 Arbol de decision, Random Forest y Naive Bayes

Se utiliza la misma variablee respuesta y los mismos predictores que el primer modelo.
```{r}
    porcentaje <- 0.7
    set.seed(1234)

    numeric_variables3 <- c("OverallQual","BsmtFinSF1","BsmtUnfSF", "X2ndFlrSF","BsmtFullBath", "FullBath", "PoolArea", "MiscVal", "clasificacion_Caras")
    datos_num <- datos_con_dummy[, numeric_variables3]

    corte <- sample(nrow(datos_num), nrow(datos_num) * porcentaje)
    train <- datos_num[corte, ]
    test <- datos_num[-corte, ]
    test_3 = select(test, -clasificacion_Caras)
```

#### 10.1 Arbol de Clasificacion

```{r}
    Rprof(memory.profiling = TRUE)
    modelo_arbol <- rpart(clasificacion_Caras~., data=train, method="class", maxdepth=4)
    rpart.plot(modelo_arbol)
    Rprof(NULL)
    
    pm4 <- summaryRprof(memory = "both")

    y_pred <- predict(modelo_arbol, test, type = "class")
    confusionMatrix(y_pred, test$clasificacion_Caras)

```

El tiempo total de este modelo fue de `r pm4$sampling.time` ms y su consumo total de memoria fue `r sum(pm4$by.total$mem.total)` MB. El accuracy de este modelo fue de 0.86. Tuvo un total de 32 falsos positivos y 12 falsos negativos.

#### 10.2 Random Forest
```{r}
    Rprof(memory.profiling = TRUE)
    modeloRF <- randomForest(clasificacion_Caras~.,train,na.action = na.omit)
    Rprof(NULL)

    pm5 <- summaryRprof(memory = "both")
    ypred <- predict(modeloRF,newdata = test)
    ypred <- factor(ypred)

    confusionMatrix(ypred,test$clasificacion_Caras)

```

El tiempo total de este modelo fue de `r pm5$sampling.time` ms y su consumo total de memoria fue `r sum(pm5$by.total$mem.total)` MB. El accuracy de este modelo fue de 0.90. Tuvo un total de 16 falsos positivos y 22 falsos negativos.

#### 10.3 Naive Bayes
```{r}
    Rprof(memory.profiling = TRUE)
    modeloNB <- naiveBayes(clasificacion_Caras~.,train)
    Rprof(NULL)

    pm6 <- summaryRprof(memory = "both")
    ypred <- predict(modeloNB,newdata = test)
    ypred <- factor(ypred)

    confusionMatrix(ypred,test$clasificacion_Caras)

```

El tiempo total de este modelo fue de `r pm6$sampling.time` ms y su consumo total de memoria fue `r sum(pm6$by.total$mem.total)` MB. El accuracy de este modelo fue de 0.84. Tuvo un total de 19 falsos positivos y 42 falsos negativos.

## 11 Comparacion de los modelos

Podemos observar que el modelo de Random Forest es el que mejor desempeño tuvo con un accuracy de 0.90. El modelo de Naive Bayes tuvo un accuracy de 0.84 y el modelo de Arbol de Clasificacion tuvo un accuracy de 0.86. El que más tardó fue el Random Forest con un tiempo de `r pm5$sampling.time`. El arbol de Clasificacion tuvo la mayor cantidad de falsos positivos con un total de 32, mientras que el Naive Bayes tuvo la mayor cantidad de falsos negativos con un total de 42. El modelo que menos se equivocó fue el Random Forest, cosa que se puede observar tanto en la matriz de confusión como en el accuracy.  
Como se puede obserevar el que mejor accuracy tuvo y el que tuvo mejor desempeño sobre los otros fue también el que más se tardó, indicando que es un modelo más robusto y que se ajusta mejor a las variables que se están utilizando para predecir.